{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b5465aaee989>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "import numpy as np\n",
    "import scipy.stats as scs\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "import sklearn.mixture as mix\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "%watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas 0.20.2\n",
      "numpy 1.12.1\n",
      "scipy 0.19.0\n",
      "sklearn 0.18.1\n",
      "matplotlib 2.0.2\n",
      "seaborn 0.7.1\n"
     ]
    }
   ],
   "source": [
    "%watermark -p pandas,numpy,scipy,sklearn,matplotlib,seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To gain understanding of mixture models we have to start at the beginning with the expectation maximization algorithm and it's application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First a little history on E-M\n",
    "\n",
    "Reference: 4\n",
    "\n",
    "Demptser, Laird & Rubin (1977)\n",
    "    -unified previously unrelated work under \"The EM Algorithm\"\n",
    "    - unified previously unrelated work under \"The EM Algorithm\"\n",
    "    - overlooked E-M works - see gaps between foundational authors\n",
    "        - Newcomb (1887)\n",
    "        - McKendrick (1926) [+39 years]\n",
    "        - Hartley (1958) [+32 years]\n",
    "        - Baum et. al. (1970) [+12 years]\n",
    "        - Dempters et. al. (1977) [+7 years]\n",
    "\n",
    "#### *EM Algorithm developed over 90 years*\n",
    "\n",
    "## EM provides general framework for solving problems\n",
    "\n",
    "Examples include:\n",
    "    - Filling in missing data from a sample set\n",
    "    - Discovering values of latent variables\n",
    "    - Estimating parameters of HMMs\n",
    "    - Estimating parameters of finite mixtures [models]\n",
    "    - Unsupervised learning of clusters\n",
    "    - etc...\n",
    "    \n",
    "### How does the EM algorithm work? \n",
    "\n",
    "EM is an iterative process that begins with a \"naive\" or random initialization and then alternates between the expectation and maximization steps until the algorithm reaches convergence. \n",
    "\n",
    "To describe this in words imagine we have a simple data set consisting of class heights with groups separated by gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Height (in)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Male</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Male</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Female</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Female</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Female</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Male</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Female</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Female</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Female</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Male</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Male</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Female</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Male</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Female</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Female</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Female</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Male</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Female</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Female</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Female</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Female</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Gender  Height (in)\n",
       "0     Male           72\n",
       "1     Male           72\n",
       "2   Female           63\n",
       "3   Female           62\n",
       "4   Female           62\n",
       "5     Male           73\n",
       "6   Female           64\n",
       "7   Female           63\n",
       "8   Female           67\n",
       "9     Male           71\n",
       "10    Male           72\n",
       "11  Female           63\n",
       "12    Male           71\n",
       "13  Female           67\n",
       "14  Female           62\n",
       "15  Female           63\n",
       "16    Male           66\n",
       "17  Female           60\n",
       "18  Female           68\n",
       "19  Female           65\n",
       "20  Female           64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import class heights\n",
    "f = 'https://raw.githubusercontent.com/BlackArbsCEO/Mixture_Models/K-Means%2C-E-M%2C-Mixture-Models/Class_heights.csv'\n",
    "\n",
    "data = pd.read_csv(f)\n",
    "# data.info()\n",
    "\n",
    "height = data['Height (in)']\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now imagine that we did not have the convenient gender labels associated with each data point. How could we estimate the two group means? \n",
    "\n",
    "First let's set up our problem. \n",
    "\n",
    "In this example we hypothesize that these height data points are drawn from two distributions with two means - < $\\mu_1$, $\\mu_2$ >.\n",
    "\n",
    "The heights are the observed $x$ values. \n",
    "\n",
    "The hidden variables, which EM is going to estimate, can be thought of in the following way. Each $x$ value has 2 associated $z$ values. These $z$ values < $z_1$, $z_2$ > represent the distribution (or class or cluster) that the data point is drawn from.\n",
    "\n",
    "Understanding the range of values the $z$ values can take is important. \n",
    "\n",
    "In k-means, the two $z$'s can only take the values of 0 or 1. If the $x$ value came from the first distribution (cluster), then $z_1$=1 and $z_2$=0 and vice versa. This is called **hard** clustering. \n",
    "\n",
    "In Gaussian Mixture Models, the $z$'s can take on any value between 0 and 1 because the x values are considered to be drawn probabilistically from 1 of the 2 distributions. For example $z$ values can be $z_1$=0.85 and $z_2$>=0.15, which represents a strong probability that the $x$ value came from distribution 1 and smaller probability that it came from distribution 2. This is called **soft** or **fuzzy** clustering. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will assume the x values are drawn from Gaussian distributions. \n",
    "\n",
    "To start the algorithm, we choose two random means. \n",
    "\n",
    "From there we repeat the following until convergence. \n",
    "\n",
    "#### The expectation step:\n",
    "\n",
    "We calculate the expected values $E(z_{ij})$, which is the probability that $x_i$ was drawn from the $jth$ distribution.\n",
    "    \n",
    "$$E(z_{ij}) = \\frac{p(x = x_i|\\mu = \\mu_j)}{\\sum_{n=1}^2 p(x = x_i|\\mu = \\mu_j)}$$\n",
    "\n",
    "\n",
    "$$= \\frac{ e^{-\\frac{1}{2\\sigma^2}(x_i - \\mu_j)^2} }\n",
    "{ \\sum_{n=1}^2e^{-\\frac{1}{2\\sigma^2}(x_i - \\mu_n)^2} }$$\n",
    "\n",
    "The formula simply states that the expected value for $z_{ij}$ is the probability $x_i$ given $\\mu_j$ divided by the sum of the probabilities that $x_i$ belonged to each $\\mu$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The maximization step:\n",
    "\n",
    "After calculating all $E(z_{ij})$ values we can calculate (update) new $\\mu$ values. \n",
    "\n",
    "$$ \\mu_j = \\frac {\\sum_{i=1}^mE(z_{ij})x_i} {\\sum_{i=1}^mE(z_{ij})}$$\n",
    "\n",
    "This formula generates the maximum likelihood estimate. \n",
    "\n",
    "By repeating the E-step and M-step we are guaranteed to find a local maximum giving us a maximum likelihood estimation of our hypothesis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Maximum Likelihood Estimates (MLE)\n",
    "\n",
    "    1. Parameters describe characteristics (attributes) of a population. These parameter values are estimated from samples collected from that population.\n",
    "\n",
    "    2. A MLE is a parameter estimate that is most consistent with the sampled data. By definition it maximizes the likelihood function. One way to accomplish this is to take the first derivative of the likelihood function w/ respect to the parameter theta and solve for 0. This value maximizes the likelihood function and is the MLE\n",
    "\n",
    "### A quick example of a maximum likelihood estimate\n",
    "    \n",
    "#### You flip a coin 10 times and observe the following sequence (H, T, T, H, T, T, T, T, H, T)\n",
    "#### What's the MLE of observing 3 heads in 10 trials?    \n",
    "\n",
    "#### simple answer:    \n",
    "    The frequentist MLE is (# of successes) / (# of trials) or 3/10\n",
    "\n",
    "#### solving first derivative of binomial distribution answer:    \n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal L(\\theta) & = {10 \\choose 3}\\theta^3(1-\\theta)^7 \\\\[1ex]\n",
    "log\\mathcal L(\\theta) & = log{10 \\choose 3} + 3log\\theta + 7log(1 - \\theta) \\\\[1ex]\n",
    "\\frac{dlog\\mathcal L(\\theta)}{d(\\theta)} & = \\frac 3\\theta - \\frac{7}{1-\\theta} = 0 \\\\[1ex]\n",
    "\\frac 3\\theta & = \\frac{7}{1 - \\theta} \\Rightarrow \\frac{3}{10}\n",
    "\\end{align}\n",
    "\n",
    "#### That's a MLE! This is the estimate that is most consistent with the observed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to our height example. Using the generalized Gaussian mixture model code sourced from Duke's computational statistics we can visualize this process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code sourced from:\n",
    "#    http://people.duke.edu/~ccc14/sta-663/EMAlgorithm.html\n",
    "\n",
    "def em_gmm_orig(xs, pis, mus, sigmas, tol=0.01, max_iter=100):\n",
    "\n",
    "    n, p = xs.shape\n",
    "    k = len(pis)\n",
    "\n",
    "    ll_old = 0\n",
    "    for i in range(max_iter):\n",
    "        print('\\nIteration: ', i)\n",
    "        print()\n",
    "        exp_A = []\n",
    "        exp_B = []\n",
    "        ll_new = 0\n",
    "\n",
    "        # E-step\n",
    "        ws = np.zeros((k, n))\n",
    "        for j in range(len(mus)):\n",
    "            for i in range(n):\n",
    "                ws[j, i] = pis[j] * mvn(mus[j], sigmas[j]).pdf(xs[i])\n",
    "        ws /= ws.sum(0)\n",
    "\n",
    "        # M-step\n",
    "        pis = np.zeros(k)\n",
    "        for j in range(len(mus)):\n",
    "            for i in range(n):\n",
    "                pis[j] += ws[j, i]\n",
    "        pis /= n\n",
    "\n",
    "        mus = np.zeros((k, p))\n",
    "        for j in range(k):\n",
    "            for i in range(n):\n",
    "                mus[j] += ws[j, i] * xs[i]\n",
    "            mus[j] /= ws[j, :].sum()\n",
    "        \n",
    "        sigmas = np.zeros((k, p, p))\n",
    "        for j in range(k):\n",
    "            for i in range(n):\n",
    "                ys = np.reshape(xs[i]- mus[j], (2,1))\n",
    "                sigmas[j] += ws[j, i] * np.dot(ys, ys.T)\n",
    "            sigmas[j] /= ws[j,:].sum()\n",
    "\n",
    "        new_mus = (np.diag(mus)[0], np.diag(mus)[1])\n",
    "        new_sigs = (np.unique(np.diag(sigmas[0]))[0], np.unique(np.diag(sigmas[1]))[0])\n",
    "        df = (pd.DataFrame(index=[1, 2]).assign(mus = new_mus).assign(sigs = new_sigs))\n",
    "        \n",
    "        xx = np.linspace(0, 100, 100)\n",
    "        yy = scs.multivariate_normal.pdf(xx, mean=new_mus[0], cov=new_sigs[0])\n",
    "        \n",
    "        colors = sns.color_palette('Dark2', 3)\n",
    "        fig, ax = plt.subplots(figsize=(9, 7))\n",
    "        ax.set_ylim(-0.001, np.max(yy))\n",
    "        ax.plot(xx, yy, color=colors[1])\n",
    "        ax.axvline(new_mus[0], ymin=0., color=colors[1])\n",
    "        ax.fill_between(xx, 0, yy, alpha=0.5, color=colors[1])\n",
    "        lo, hi = ax.get_ylim()\n",
    "        ax.annotate(f'$\\mu_1$: {new_mus[0]:3.2f}', \n",
    "                    fontsize=12, fontweight='demi',\n",
    "                    xy=(new_mus[0], (hi-lo) / 2), \n",
    "                    xycoords='data', xytext=(80, (hi-lo) / 2),\n",
    "                    arrowprops=dict(facecolor='black', connectionstyle=\"arc3,rad=0.2\",shrink=0.05))\n",
    "        ax.fill_between(xx, 0, yy, alpha=0.5, color=colors[2])\n",
    "        \n",
    "        yy2 = scs.multivariate_normal.pdf(xx, mean=new_mus[1], cov=new_sigs[1])\n",
    "        \n",
    "        ax.plot(xx, yy2, color=colors[2])\n",
    "        ax.axvline(new_mus[1], ymin=0., color=colors[2])\n",
    "        lo, hi = ax.get_ylim()\n",
    "        ax.annotate(f'$\\mu_2$: {new_mus[1]:3.2f}', \n",
    "                    fontsize=12, fontweight='demi',\n",
    "            xy=(new_mus[1], (hi-lo) / 2), xycoords='data', xytext=(25, (hi-lo) / 2),\n",
    "            arrowprops=dict(facecolor='black', connectionstyle=\"arc3,rad=0.2\",shrink=0.05))\n",
    "        ax.fill_between(xx, 0, yy2, alpha=0.5, color=colors[2])\n",
    "        \n",
    "        dot_kwds = dict(markerfacecolor='white', markeredgecolor='black', markeredgewidth=1, markersize=10)\n",
    "        ax.plot(height, len(height)*[0], 'o', **dot_kwds)\n",
    "        ax.set_ylim(-0.001, np.max(yy2))\n",
    "        \n",
    "        \n",
    "        print(df.T)   \n",
    "        \n",
    "        # update complete log likelihoood\n",
    "        ll_new = 0.0\n",
    "        for i in range(n):\n",
    "            s = 0\n",
    "            for j in range(k):\n",
    "                s += pis[j] * mvn(mus[j], sigmas[j]).pdf(xs[i])\n",
    "            ll_new += np.log(s)\n",
    "        print(f'log_likelihood: {ll_new:3.4f}')\n",
    "        if np.abs(ll_new - ll_old) < tol:\n",
    "            break\n",
    "        ll_old = ll_new\n",
    "        \n",
    "    return ll_new, pis, mus, sigmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "height = data['Height (in)']\n",
    "n = len(height)\n",
    "\n",
    "# Ground truthish\n",
    "_mus = np.array([[0, data.groupby('Gender').mean().iat[0, 0]], \n",
    "                 [data.groupby('Gender').mean().iat[1, 0], 0]])\n",
    "_sigmas = np.array([[[5, 0], [0, 5]], \n",
    "                    [[5, 0],[0, 5]]])\n",
    "_pis = np.array([0.5, 0.5]) # priors\n",
    "\n",
    "# initial random guesses for parameters\n",
    "np.random.seed(0)\n",
    "\n",
    "pis = np.random.random(2)\n",
    "pis /= pis.sum()\n",
    "mus = np.random.random((2,2))\n",
    "sigmas = np.array([np.eye(2)] * 2) * height.std()\n",
    "\n",
    "# generate our noisy x values\n",
    "xs = np.concatenate([np.random.multivariate_normal(mu, sigma, int(pi*n))\n",
    "                    for pi, mu, sigma in zip(_pis, _mus, _sigmas)])\n",
    "\n",
    "ll, pis, mus, sigmas = em_gmm_orig(xs, pis, mus, sigmas)\n",
    "\n",
    "# In the below plots the white dots represent the observed heights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notice how the algorithm was able to estimate the true means starting from random guesses for the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have a grasp of the algorithm we can examine K-Means as a form of EM\n",
    "\n",
    "K-Means is an unsupervised learning algorithm used for clustering multidimensional data sets.\n",
    "\n",
    "The basic form of K-Means makes two assumptions\n",
    "\n",
    "    1. Each data point is closer to its own cluster center than the other cluster centers\n",
    "    2. A cluster center is the arithmetic mean of all the points that belong to the cluster.\n",
    "\n",
    "The expectation step is done by calculating the pairwise distances  of every data point and assigning cluster membership to the closest center (mean)\n",
    "\n",
    "The maximization step is simply the arithmetic mean of the previously assigned data points for each cluster\n",
    "\n",
    "#### The following sections borrow heavily from Jake Vanderplas' [Python Data Science Handbook](https://www.amazon.com/gp/product/1491912057/ref=as_li_qf_sp_asin_il_tl?ie=UTF8&tag=blkarbs-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1491912057&linkId=e000ed9627cfe6b505be11c50118decb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's define some demo variables and make some blobs\n",
    "\n",
    "# demo variables\n",
    "\n",
    "k = 4\n",
    "n_draws = 500\n",
    "sigma = .7\n",
    "random_state = 0\n",
    "dot_size = 50\n",
    "cmap = 'viridis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make blobs\n",
    "\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "X, y_true = make_blobs(n_samples = n_draws,\n",
    "                       centers = k,\n",
    "                       cluster_std = sigma,\n",
    "                       random_state = random_state)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,7))\n",
    "ax.scatter(X[:, 0], X[:, 1], s=dot_size)\n",
    "plt.title('k-means make blobs', fontsize=18, fontweight='demi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample implementation\n",
    "# code sourced from: \n",
    "#   http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.11-K-Means.ipynb\n",
    "\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "\n",
    "def find_clusters(X, n_clusters, rseed=2):\n",
    "    # 1. Random initialization (choose random clusters)\n",
    "    rng = np.random.RandomState(rseed)\n",
    "    i = rng.permutation(X.shape[0])[:n_clusters]\n",
    "    centers = X[i]\n",
    "\n",
    "    while True:\n",
    "        # 2a. Assign labels based on closest center\n",
    "        labels = pairwise_distances_argmin(X, centers)\n",
    "\n",
    "        # 2b. Find new centers from means of points\n",
    "        new_centers = np.array([X[labels == i].mean(0) \n",
    "                                for i in range(n_clusters)])\n",
    "\n",
    "        # 2c. Check for convergence\n",
    "        if np.all(centers == new_centers):\n",
    "            break\n",
    "        centers = new_centers\n",
    "    \n",
    "    return centers, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's test the implementation\n",
    "\n",
    "centers, labels = find_clusters(X, k)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,7))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=labels, s=dot_size, cmap=cmap)\n",
    "plt.title('find_clusters() k-means func', fontsize=18, fontweight='demi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now let's compare this to the sklearn's KMeans() algorithm\n",
    "\n",
    "# fit k-means to blobs\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)\n",
    "\n",
    "# visualize prediction\n",
    "fig, ax = plt.subplots(figsize=(9,7))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=dot_size, cmap=cmap)\n",
    "\n",
    "# get centers for plot\n",
    "centers = kmeans.cluster_centers_\n",
    "ax.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.75)\n",
    "plt.title('sklearn k-means', fontsize=18, fontweight='demi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To build our intuition of this process, play with the following interactive code from Jake Vanderplas in an Jupyter (IPython) notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code sourced from:\n",
    "#   http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/06.00-Figure-Code.ipynb#Covariance-Type\n",
    "\n",
    "from ipywidgets import interact\n",
    "\n",
    "def plot_kmeans_interactive(min_clusters=1, max_clusters=6):\n",
    "    X, y = make_blobs(n_samples=300, centers=4,\n",
    "                      random_state=0, cluster_std=0.60)\n",
    "        \n",
    "    def plot_points(X, labels, n_clusters):\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis',\n",
    "                    vmin=0, vmax=n_clusters - 1);\n",
    "            \n",
    "    def plot_centers(centers):\n",
    "        plt.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                    c=np.arange(centers.shape[0]),\n",
    "                    s=200, cmap='viridis')\n",
    "        plt.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                    c='black', s=50)\n",
    "            \n",
    "\n",
    "    def _kmeans_step(frame=0, n_clusters=4):\n",
    "        rng = np.random.RandomState(2)\n",
    "        labels = np.zeros(X.shape[0])\n",
    "        centers = rng.randn(n_clusters, 2)\n",
    "\n",
    "        nsteps = frame // 3\n",
    "\n",
    "        for i in range(nsteps + 1):\n",
    "            old_centers = centers\n",
    "            if i < nsteps or frame % 3 > 0:\n",
    "                labels = pairwise_distances_argmin(X, centers)\n",
    "\n",
    "            if i < nsteps or frame % 3 > 1:\n",
    "                centers = np.array([X[labels == j].mean(0)\n",
    "                                    for j in range(n_clusters)])\n",
    "                nans = np.isnan(centers)\n",
    "                centers[nans] = old_centers[nans]\n",
    "\n",
    "        # plot the data and cluster centers\n",
    "        plot_points(X, labels, n_clusters)\n",
    "        plot_centers(old_centers)\n",
    "\n",
    "        # plot new centers if third frame\n",
    "        if frame % 3 == 2:\n",
    "            for i in range(n_clusters):\n",
    "                plt.annotate('', centers[i], old_centers[i], \n",
    "                             arrowprops=dict(arrowstyle='->', linewidth=1))\n",
    "            plot_centers(centers)\n",
    "\n",
    "        plt.xlim(-4, 4)\n",
    "        plt.ylim(-2, 10)\n",
    "\n",
    "        if frame % 3 == 1:\n",
    "            plt.text(3.8, 9.5, \"1. Reassign points to nearest centroid\",\n",
    "                     ha='right', va='top', size=14)\n",
    "        elif frame % 3 == 2:\n",
    "            plt.text(3.8, 9.5, \"2. Update centroids to cluster means\",\n",
    "                     ha='right', va='top', size=14)\n",
    "    \n",
    "    return interact(_kmeans_step, frame=[0, 50],\n",
    "                    n_clusters=[min_clusters, max_clusters])\n",
    "\n",
    "plot_kmeans_interactive();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we are ready to explore some of the nuances/issues of implementing K-Means as an expectation maximization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the globally optimal result is not guaranteed\n",
    "    - EM is guaranteed to improve the result in each iteration but there are no guarantees that it will find the global best. See the following example, where we initalize the algorithm with a different seed.\n",
    "\n",
    "### practical solution: \n",
    "    - Run the algorithm w/ multiple random initializations\n",
    "    - This is done by default in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centers, labels = find_clusters(X, k, rseed=11)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,7))\n",
    "ax.set_title('sub-optimal clustering', fontsize=18, fontweight='demi')\n",
    "ax.scatter(X[:, 0], X[:, 1], c=labels, s=dot_size, cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### number of means (clusters) have to be selected beforehand\n",
    "    - k-means cannot learn the optimal number of clusters from the data. If we ask for six clusters it will find six clusters, which may or may not be meaningful.\n",
    "    \n",
    "### practical solution:\n",
    "    - use a more complex clustering algorithm like Gaussian Mixture Models, or one that can choose a suitable number of clusters (DBSCAN, mean-shift, affinity propagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels6 = KMeans(6, random_state=random_state).fit_predict(X)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,7))\n",
    "ax.set_title('too many clusters', fontsize=18, fontweight='demi')\n",
    "ax.scatter(X[:, 0], X[:, 1], c=labels6, s=dot_size, cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means is terrible for non-linear data:\n",
    "    - this results because of the assumption that points will be closer to their own cluster center than others\n",
    "\n",
    "### practical solutions:\n",
    "    - transform data into higher dimension where linear separation is possible e.g., spectral clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X_mn, y_mn = make_moons(500, noise=.07, random_state=random_state)\n",
    "\n",
    "labelsM = KMeans(2, random_state=random_state).fit_predict(X_mn)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,7))\n",
    "ax.set_title('linear separation not possible', fontsize=18, fontweight='demi')\n",
    "ax.scatter(X_mn[:, 0], X_mn[:, 1], c=labelsM, s=dot_size, cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "model = SpectralClustering(n_clusters=2, affinity='nearest_neighbors',\n",
    "                          assign_labels='kmeans')\n",
    "\n",
    "labelsS = model.fit_predict(X_mn)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,7))\n",
    "ax.set_title('kernal transform to higher dimension\\nlinear separation is possible', fontsize=18, fontweight='demi')\n",
    "plt.scatter(X_mn[:, 0], X_mn[:, 1], c=labelsS, s=dot_size, cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means is known as a hard clustering algorithm because clusters are not allowed to overlap.  \n",
    "\n",
    "> ___\"One way to think about the k-means model is that it places a circle (or, in higher dimensions, a hyper-sphere) at the center of each cluster, with a radius defined by the most distant point in the cluster. This radius acts as a hard cutoff for cluster assignment within the training set: any point outside this circle is not considered a member of the cluster.___ -- <cite> [Jake VanderPlas Python Data Science Handbook] [1]</cite>\n",
    "\n",
    "[1]:http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.12-Gaussian-Mixtures.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# k-means weaknesses that mixture models address directly\n",
    "# code sourced from:\n",
    "#   http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.12-Gaussian-Mixtures.ipynb\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def plot_kmeans(kmeans, X, n_clusters=k, rseed=2, ax=None):\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    \n",
    "    # plot input data\n",
    "    #ax = ax or plt.gca() # <-- nice trick\n",
    "    fig, ax = plt.subplots(figsize=(9,7))    \n",
    "    ax.axis('equal')\n",
    "    ax.scatter(X[:, 0], X[:, 1],\n",
    "               c=labels, s=dot_size, cmap=cmap, zorder=2)\n",
    "    \n",
    "    # plot the representation of Kmeans model\n",
    "    centers = kmeans.cluster_centers_\n",
    "    radii = [cdist(X[labels==i], [center]).max() \n",
    "             for i, center in enumerate(centers)]\n",
    "    \n",
    "    for c, r in zip(centers, radii):\n",
    "        ax.add_patch(plt.Circle(c, r, fc='#CCCCCC',edgecolor='slategrey',\n",
    "                                lw=4, alpha=0.5, zorder=1))\n",
    "    return      \n",
    "                     \n",
    "X3, y_true = make_blobs(n_samples = 400,\n",
    "                       centers = k,\n",
    "                       cluster_std = .6,\n",
    "                       random_state = random_state)\n",
    "X3 = X3[:, ::-1] # better plotting\n",
    "\n",
    "kmeans = KMeans(n_clusters=k, random_state=random_state)\n",
    "plot_kmeans(kmeans, X3)\n",
    "plt.title('Clusters are hard circular boundaries', fontsize=18, fontweight='demi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A resulting issue of K-Means' circular boundaries is that it has no way to account for oblong or elliptical clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(13)\n",
    "X3_stretched = np.dot(X3, rng.randn(2, 2))\n",
    "\n",
    "kmeans = KMeans(n_clusters=k, random_state=random_state)\n",
    "plot_kmeans(kmeans, X3_stretched)\n",
    "plt.title('Clusters cannot adjust to elliptical data structures',\n",
    "         fontsize=18, fontweight='demi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are two ways we can extend K-Means\n",
    "\n",
    "    1. measure uncertainty in cluster assignments by comparing distances to all cluster centers\n",
    "    2. allow for flexibility in the shape of the cluster boundaries by using ellipses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall our previous height example, and let's  assume that each cluster is a Gaussian distribution!\n",
    "\n",
    "#### Gaussian distributions give flexibility to the clustering, and the same basic two step E-M algorithm used in K-Means is applied here as well. \n",
    "\n",
    "1. Randomly initialize location and shape\n",
    "2. Repeat until converged:\n",
    "       E-step: for each point, find weights encoding the probability of membership in each cluster.\n",
    "       \n",
    "       M-step: for each cluster, update its location, normalization, and shape based on all data points, making use of the weights\n",
    "       \n",
    "#### The result of this process is that we end up with a smooth Gaussian cluster better fitted to the shape of the data, instead of a rigid inflexible circle. \n",
    "\n",
    "#### Note that because we still are using the E-M algorithm there is no guarantee of a globally optimal result. We can visualize the results of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code sourced from:\n",
    "#  http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.12-Gaussian-Mixtures.ipynb\n",
    "from matplotlib.patches import Ellipse \n",
    "\n",
    "def draw_ellipse(position, covariance, ax=None, **kwargs):\n",
    "    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n",
    "    \n",
    "    # Convert covariance to principal axes\n",
    "    if covariance.shape == (2, 2):\n",
    "        U, s, Vt = np.linalg.svd(covariance)\n",
    "        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
    "        width, height = 2 * np.sqrt(s)\n",
    "    else:\n",
    "        angle = 0\n",
    "        width, height = 2 * np.sqrt(covariance)\n",
    "        \n",
    "    # Draw the Ellipse\n",
    "    for nsig in range(1, 4):\n",
    "        ax.add_patch(Ellipse(position, nsig * width, nsig * height, \n",
    "                            angle, **kwargs))\n",
    "        \n",
    "def plot_gmm(gmm, X, label=True, ax=None):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(9,7))      \n",
    "    ax = ax or plt.gca()\n",
    "    labels = gmm.fit(X).predict(X)\n",
    "    \n",
    "    if label:\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=labels, s=dot_size, cmap=cmap, zorder=2)\n",
    "    else:\n",
    "        ax.scatter(X[:, 0], X[:, 1], s=dot_size, zorder=2)\n",
    "    ax.axis('equal')\n",
    "    \n",
    "    w_factor = 0.2 / gmm.weights_.max()\n",
    "    for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):\n",
    "        draw_ellipse(pos, covar, ax=ax, alpha=w * w_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gmm = mix.GaussianMixture(n_components=k, random_state=random_state)\n",
    "plot_gmm(gmm, X3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets test on the stretched data set\n",
    "\n",
    "gmm = mix.GaussianMixture(n_components=k, random_state=random_state+1)\n",
    "plot_gmm(gmm, X3_stretched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notice how much better the model is able to fit the clusters when we assume each cluster is a Gaussian distribution instead of circle whose radius is defined by the most distant point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Models as a tool for Density Estimation\n",
    "\n",
    "#### The technical term for this type of model is: \n",
    "\n",
    "> __generative probabilistic model__\n",
    "\n",
    "#### Why you ask? \n",
    "\n",
    "Because this model is really about characterizing the distribution of the entire dataset and not necessarily clustering. The power of these types of models is that they allow us to generate __new__ samples that __mimic__ the original underlying data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gmm2 = mix.GaussianMixture(n_components=2, covariance_type='full',\n",
    "                          random_state=random_state)\n",
    "plot_gmm(gmm2, X_mn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to cluster this data set we run into the same issue as before.\n",
    "\n",
    "Instead let's ignore individual clusters and model the whole distribution of data as a collection of many Gaussians. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gmm16 = mix.GaussianMixture(n_components=16, covariance_type='full', \n",
    "                           random_state=random_state)\n",
    "\n",
    "plot_gmm(gmm16, X_mn, label=False)\n",
    "plt.title('Collective Gaussian clusters',\n",
    "            fontsize=18, fontweight='demi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the collection of clusters has fit the data set reasonably well. Now let's see if the model has actually _learned_ about this data set, such that we can create entirely new samples that _look_ like the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xnew, ynew = gmm16.sample(500)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,7))\n",
    "ax.scatter(Xnew[:, 0], Xnew[:, 1]);\n",
    "ax.set_title('New samples drawn from fitted model',\n",
    "            fontsize=18, fontweight='demi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative models allow for multiple methods to determine optimal number of components. Because it is a probability distribution we can evaluate the likelihood of the data using cross validation and/or using AIC or BIC. \n",
    "\n",
    "Sklearn makes this easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_components = np.arange(1, 21)\n",
    "models = [mix.GaussianMixture(n, covariance_type='full',\n",
    "                             random_state=random_state).fit(X_mn)\n",
    "         for n in n_components]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,7))\n",
    "ax.plot(n_components, [m.bic(X_mn) for m in models], label='BIC')\n",
    "ax.plot(n_components, [m.aic(X_mn) for m in models], label='AIC')\n",
    "ax.axvline(np.argmin([m.bic(X_mn) for m in models]), color='blue')\n",
    "ax.axvline(np.argmin([m.aic(X_mn) for m in models]), color='green')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('n_components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "state": {
    "d770be2293824ed7a3e445ce17c81805": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
